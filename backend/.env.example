# Backend Configuration
PORT=4000
HOST=127.0.0.1

# Hugging Face API configuration
HF_API_TOKEN=your_huggingface_token_here
# Optional preferred model (leave blank to auto-select from your account's supported models)
HF_MODEL=
# Optional prioritized model list (comma-separated)
# HF_MODELS=meta-llama/Llama-3.1-8B-Instruct,Qwen/Qwen3.5-32B
# Optional per-agent model overrides
# HF_NUTRITION_MODEL=prathch2/nutrition_openfoodfacts_rag
# HF_MEDS_MODEL=epfl-llm/meditron-7b
# Optional endpoint override
# HF_API_URL=https://router.huggingface.co/v1/chat/completions

# AI provider selection: auto | huggingface | github
# AI_PROVIDER=auto

# GitHub Models API (OpenAI-compatible)
# GITHUB_MODELS_API_KEY=your_github_models_key_here
# Optional alias key name supported by backend
# GITHUB_TOKEN=your_github_models_key_here
# GITHUB_API_KEY=your_github_models_key_here
# GITHUB_MODEL_API_KEY=your_github_models_key_here
# Optional endpoint override
# GITHUB_MODELS_API_URL=https://models.inference.ai.azure.com/chat/completions
# Optional default and routed models
# GITHUB_MODEL=gpt-4o-mini
# GITHUB_MODELS=gpt-4o-mini,meta-llama/Meta-Llama-3.1-8B-Instruct
# GITHUB_NUTRITION_MODEL=gpt-4o-mini
# GITHUB_MEDS_MODEL=gpt-4o-mini

# JWT Secret for authentication
JWT_SECRET=your_jwt_secret_here
